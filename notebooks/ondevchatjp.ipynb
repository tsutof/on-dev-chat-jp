{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPlxJidgT6AUzDpjWSHydv+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!apt update && apt install -y ffmpeg portaudio19-dev"],"metadata":{"id":"nHBcVyCEe36q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706739226751,"user_tz":-540,"elapsed":11143,"user":{"displayName":"T F","userId":"15651270806090796677"}},"outputId":"122aebd1-5c74-423a-c559-a09ca2040bae"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [1 InRelease 5,481 B/110 kB 5%] [Connecting to \u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to cloud.r-project.org] [Waiting fo\u001b[0m\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","\u001b[33m\r                                                                                                    \r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Waiting for headers]\u001b[0m\r                                                                                  \rHit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","\u001b[33m\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Waiting for headers]\u001b[0m\r                                                                                  \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","\u001b[33m\r                                                                                  \r0% [Waiting for headers] [Connecting to cloud.r-project.org]\u001b[0m\r                                                            \rGet:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n","\u001b[33m\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [5 InRelease 24.3 kB/24.3 kB 100%]\u001b[0m\u001b[33m\r                                                                                               \r0% [Waiting for headers] [Connecting to cloud.r-project.org]\u001b[0m\r                                                            \rHit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,063 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,398 kB]\n","Get:11 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [44.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,677 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,333 kB]\n","Fetched 5,773 kB in 2s (3,403 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","34 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","The following additional packages will be installed:\n","  libportaudio2 libportaudiocpp0\n","Suggested packages:\n","  portaudio19-doc\n","The following NEW packages will be installed:\n","  libportaudio2 libportaudiocpp0 portaudio19-dev\n","0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n","Need to get 188 kB of archives.\n","After this operation, 927 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n","Fetched 188 kB in 1s (345 kB/s)\n","Selecting previously unselected package libportaudio2:amd64.\n","(Reading database ... 121730 files and directories currently installed.)\n","Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n","Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n","Selecting previously unselected package libportaudiocpp0:amd64.\n","Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n","Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n","Selecting previously unselected package portaudio19-dev:amd64.\n","Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n","Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n","Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n","Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n","Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n"]}]},{"cell_type":"code","source":["!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"],"metadata":{"id":"IGCAth5DfWlA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706739360433,"user_tz":-540,"elapsed":116277,"user":{"displayName":"T F","userId":"15651270806090796677"}},"outputId":"e34eea7b-c64e-4858-b0d5-7355de7e5994"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.38.tar.gz (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n","Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.38-cp310-cp310-manylinux_2_35_x86_64.whl size=9660857 sha256=ab2e892051b3108a2e374ed4686be0f9518a360fd6ca8f98b9f5b23006a32da7\n","  Stored in directory: /root/.cache/pip/wheels/eb/58/77/20d3d9a235b4930050fbcde1ad4f0a4d054644269e801b08aa\n","Successfully built llama-cpp-python\n","Installing collected packages: llama-cpp-python\n","Successfully installed llama-cpp-python-0.2.38\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/tsutof/on-dev-chat-jp"],"metadata":{"id":"dlYB9jTslDto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip on-dev-chat-jp-main.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ToIF1bNCl2CV","executionInfo":{"status":"ok","timestamp":1706739376601,"user_tz":-540,"elapsed":587,"user":{"displayName":"T F","userId":"15651270806090796677"}},"outputId":"1c1025c4-2e17-49be-dac6-ad80efceda62"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  on-dev-chat-jp-main.zip\n","239077b3e06a3bed93226f0de8c5feec5540ed2a\n","   creating: on-dev-chat-jp-main/\n","  inflating: on-dev-chat-jp-main/.gitignore  \n","   creating: on-dev-chat-jp-main/.vscode/\n","  inflating: on-dev-chat-jp-main/.vscode/launch.json  \n","  inflating: on-dev-chat-jp-main/LICENSE  \n","  inflating: on-dev-chat-jp-main/README.md  \n","   creating: on-dev-chat-jp-main/images/\n","  inflating: on-dev-chat-jp-main/images/free_chat.png  \n","  inflating: on-dev-chat-jp-main/images/rag_chat.png  \n","  inflating: on-dev-chat-jp-main/images/vector_search.png  \n","   creating: on-dev-chat-jp-main/ondevchatjp/\n","  inflating: on-dev-chat-jp-main/ondevchatjp/__init__.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/app_args.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/chain.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/free_chat.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/model.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/rag_chat.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/transcriber.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/vector_search.py  \n","  inflating: on-dev-chat-jp-main/ondevchatjp/vector_store.py  \n","  inflating: on-dev-chat-jp-main/setup.py  \n","   creating: on-dev-chat-jp-main/tests/\n","  inflating: on-dev-chat-jp-main/tests/test_rag.py  \n"]}]},{"cell_type":"code","source":["# !pip install -e on-dev-chat-jp/\n","!pip install -e on-dev-chat-jp-main/"],"metadata":{"id":"e107ULHUf-Sl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706739599968,"user_tz":-540,"elapsed":152852,"user":{"displayName":"T F","userId":"15651270806090796677"}},"outputId":"c56d16a7-c9e8-4440-a58f-2f27eb8500d0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/on-dev-chat-jp-main\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ondevchatjp==0.0.1) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from ondevchatjp==0.0.1) (0.16.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from ondevchatjp==0.0.1) (2.1.0+cu121)\n","Collecting pyopenjtalk (from ondevchatjp==0.0.1)\n","  Downloading pyopenjtalk-0.3.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting openai-whisper (from ondevchatjp==0.0.1)\n","  Downloading openai-whisper-20231117.tar.gz (798 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyaudio (from ondevchatjp==0.0.1)\n","  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio (from ondevchatjp==0.0.1)\n","  Downloading gradio-4.16.0-py3-none-any.whl (16.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith (from ondevchatjp==0.0.1)\n","  Downloading langsmith-0.0.85-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain (from ondevchatjp==0.0.1)\n","  Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-community (from ondevchatjp==0.0.1)\n","  Downloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chromadb (from ondevchatjp==0.0.1)\n","  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bs4 (from ondevchatjp==0.0.1)\n","  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n","Collecting sentence_transformers (from ondevchatjp==0.0.1)\n","  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (from ondevchatjp==0.0.1) (0.2.38)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->ondevchatjp==0.0.1) (4.11.2)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (1.0.3)\n","Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (2.31.0)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (1.10.14)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (1.23.5)\n","Collecting posthog>=2.4.0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading posthog-3.3.4-py2.py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (4.5.0)\n","Collecting pulsar-client>=3.1.0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (0.15.1)\n","Collecting pypika>=0.48.9 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (4.66.1)\n","Collecting overrides>=7.3.1 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (6.1.1)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (1.60.0)\n","Collecting bcrypt>=4.0.1 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (0.9.0)\n","Collecting kubernetes>=28.1.0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (8.2.3)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->ondevchatjp==0.0.1) (6.0.1)\n","Collecting mmh3>=4.0.1 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio->ondevchatjp==0.0.1)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (4.2.2)\n","Collecting ffmpy (from gradio->ondevchatjp==0.0.1)\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.8.1 (from gradio->ondevchatjp==0.0.1)\n","  Downloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio->ondevchatjp==0.0.1)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (0.20.3)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (3.1.3)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (2.1.4)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (3.7.1)\n","Collecting orjson~=3.0 (from gradio->ondevchatjp==0.0.1)\n","  Downloading orjson-3.9.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (23.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (1.5.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->ondevchatjp==0.0.1) (9.4.0)\n","Collecting pydantic>=1.9 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydub (from gradio->ondevchatjp==0.0.1)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart (from gradio->ondevchatjp==0.0.1)\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruff>=0.1.7 (from gradio->ondevchatjp==0.0.1)\n","  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio->ondevchatjp==0.0.1)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio->ondevchatjp==0.0.1)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.1->gradio->ondevchatjp==0.0.1) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.8.1->gradio->ondevchatjp==0.0.1)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ondevchatjp==0.0.1) (2.0.24)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->ondevchatjp==0.0.1) (3.9.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ondevchatjp==0.0.1) (4.0.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->ondevchatjp==0.0.1)\n","  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain->ondevchatjp==0.0.1)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langchain-core<0.2,>=0.1.16 (from langchain->ondevchatjp==0.0.1)\n","  Downloading langchain_core-0.1.17-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python->ondevchatjp==0.0.1) (5.6.3)\n","Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper->ondevchatjp==0.0.1) (2.1.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper->ondevchatjp==0.0.1) (0.58.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper->ondevchatjp==0.0.1) (10.1.0)\n","Collecting tiktoken (from openai-whisper->ondevchatjp==0.0.1)\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyopenjtalk->ondevchatjp==0.0.1) (1.16.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->ondevchatjp==0.0.1) (4.35.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->ondevchatjp==0.0.1) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->ondevchatjp==0.0.1) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->ondevchatjp==0.0.1) (3.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->ondevchatjp==0.0.1) (0.1.99)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ondevchatjp==0.0.1) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ondevchatjp==0.0.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ondevchatjp==0.0.1) (3.2.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ondevchatjp==0.0.1) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ondevchatjp==0.0.1) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ondevchatjp==0.0.1) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ondevchatjp==0.0.1) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ondevchatjp==0.0.1) (1.3.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->ondevchatjp==0.0.1) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->ondevchatjp==0.0.1) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->ondevchatjp==0.0.1) (0.12.1)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->ondevchatjp==0.0.1) (1.0.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->ondevchatjp==0.0.1) (2.0.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->ondevchatjp==0.0.1)\n","  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->ondevchatjp==0.0.1)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting starlette<0.36.0,>=0.35.0 (from fastapi>=0.95.2->chromadb->ondevchatjp==0.0.1)\n","  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb->ondevchatjp==0.0.1)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->ondevchatjp==0.0.1)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (2.17.3)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (1.7.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (2.0.7)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain->ondevchatjp==0.0.1) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->ondevchatjp==0.0.1) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->ondevchatjp==0.0.1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->ondevchatjp==0.0.1) (4.47.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->ondevchatjp==0.0.1) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->ondevchatjp==0.0.1) (3.1.1)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb->ondevchatjp==0.0.1)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->ondevchatjp==0.0.1) (23.5.26)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->ondevchatjp==0.0.1) (3.20.3)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n","Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->ondevchatjp==0.0.1) (1.62.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n","Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n","Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n","Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n","Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->ondevchatjp==0.0.1)\n","  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->ondevchatjp==0.0.1) (67.7.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->ondevchatjp==0.0.1) (1.14.1)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->ondevchatjp==0.0.1)\n","  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->ondevchatjp==0.0.1) (2023.3.post1)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Collecting annotated-types>=0.4.0 (from pydantic>=1.9->chromadb->ondevchatjp==0.0.1)\n","  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Collecting pydantic-core==2.16.1 (from pydantic>=1.9->chromadb->ondevchatjp==0.0.1)\n","  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb->ondevchatjp==0.0.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb->ondevchatjp==0.0.1) (3.6)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ondevchatjp==0.0.1) (3.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers->ondevchatjp==0.0.1) (2023.6.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers->ondevchatjp==0.0.1) (0.4.2)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->ondevchatjp==0.0.1) (8.1.7)\n","Collecting colorama<0.5.0,>=0.4.3 (from typer>=0.9.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer>=0.9.0->chromadb->ondevchatjp==0.0.1)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->ondevchatjp==0.0.1) (13.7.0)\n","Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb->ondevchatjp==0.0.1)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb->ondevchatjp==0.0.1)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->ondevchatjp==0.0.1)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->ondevchatjp==0.0.1)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->ondevchatjp==0.0.1)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->ondevchatjp==0.0.1) (2.5)\n","Collecting httpcore==1.* (from httpx->gradio->ondevchatjp==0.0.1)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->ondevchatjp==0.0.1) (1.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers->ondevchatjp==0.0.1) (1.3.2)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper->ondevchatjp==0.0.1) (0.41.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers->ondevchatjp==0.0.1) (3.2.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ondevchatjp==0.0.1) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain->ondevchatjp==0.0.1) (1.2.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (4.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->ondevchatjp==0.0.1) (3.17.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->ondevchatjp==0.0.1) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->ondevchatjp==0.0.1) (0.32.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->ondevchatjp==0.0.1) (0.17.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer>=0.9.0->chromadb->ondevchatjp==0.0.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer>=0.9.0->chromadb->ondevchatjp==0.0.1) (2.16.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ondevchatjp==0.0.1)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->ondevchatjp==0.0.1)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer>=0.9.0->chromadb->ondevchatjp==0.0.1) (0.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->ondevchatjp==0.0.1) (0.5.1)\n","Building wheels for collected packages: openai-whisper, pyaudio, pyopenjtalk, pypika, ffmpy\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=ffac7c479bf25a34952aadd70b2602e9ea4a573f2d8eb81263891b6628e87d77\n","  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n","  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyaudio: filename=PyAudio-0.2.14-cp310-cp310-linux_x86_64.whl size=63859 sha256=5d0e0c8207833c64a97351de0fdd66a1ffef5bc663dbd53c0f4bd90433cb06b4\n","  Stored in directory: /root/.cache/pip/wheels/d6/21/f4/0b51d41ba79e51b16295cbb096ec49f334792814d545b508c5\n","  Building wheel for pyopenjtalk (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyopenjtalk: filename=pyopenjtalk-0.3.3-cp310-cp310-linux_x86_64.whl size=5354502 sha256=7c68a19a46727cd928bd75720908759517bb76aeaf40767592c9bab6f131a77b\n","  Stored in directory: /root/.cache/pip/wheels/64/a9/5f/19eedcf7741bcd1494dc8782b6842e433314793492cc9167c9\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=e31ed5057dc95e6e2a53d715d1ff7aa56b4fd418e6d712f8e4f375735c6263b8\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=630790695d1f405c9ab343d89038ee48a683be210bdfa54198ae4e084ec5c89d\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built openai-whisper pyaudio pyopenjtalk pypika ffmpy\n","Installing collected packages: pypika, pydub, pyaudio, monotonic, mmh3, ffmpy, websockets, uvloop, typing-extensions, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, pyopenjtalk, pulsar-client, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, deprecated, colorama, chroma-hnswlib, bcrypt, backoff, annotated-types, aiofiles, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, pydantic-core, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, coloredlogs, bs4, asgiref, pydantic, opentelemetry-sdk, opentelemetry-instrumentation, openai-whisper, onnxruntime, kubernetes, httpx, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langsmith, gradio-client, fastapi, sentence_transformers, opentelemetry-instrumentation-fastapi, langchain-core, gradio, langchain-community, chromadb, langchain, ondevchatjp\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 7.0.1\n","    Uninstalling importlib-metadata-7.0.1:\n","      Successfully uninstalled importlib-metadata-7.0.1\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.14\n","    Uninstalling pydantic-1.10.14:\n","      Successfully uninstalled pydantic-1.10.14\n","  Running setup.py develop for ondevchatjp\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 colorama-0.4.6 coloredlogs-15.0.1 dataclasses-json-0.6.3 deprecated-1.2.14 fastapi-0.109.0 ffmpy-0.3.1 gradio-4.16.0 gradio-client-0.8.1 h11-0.14.0 httpcore-1.0.2 httptools-0.6.1 httpx-0.26.0 humanfriendly-10.0 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.17 langsmith-0.0.85 marshmallow-3.20.2 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 ondevchatjp-0.0.1 onnxruntime-1.17.0 openai-whisper-20231117 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.9.12 overrides-7.7.0 posthog-3.3.4 pulsar-client-3.4.0 pyaudio-0.2.14 pydantic-2.6.0 pydantic-core-2.16.1 pydub-0.25.1 pyopenjtalk-0.3.3 pypika-0.48.9 python-dotenv-1.0.1 python-multipart-0.0.6 ruff-0.1.15 semantic-version-2.10.0 sentence_transformers-2.3.1 shellingham-1.5.4 starlette-0.35.1 tiktoken-0.5.2 tomlkit-0.12.0 typing-extensions-4.9.0 typing-inspect-0.9.0 uvicorn-0.27.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"]}]},{"cell_type":"code","source":["!python -m ondevchatjp.free_chat --share"],"metadata":{"id":"c85HT2R6hUBJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706739948553,"user_tz":-540,"elapsed":201098,"user":{"displayName":"T F","userId":"15651270806090796677"}},"outputId":"c3d488b6-c95c-471d-91d0-e0c659f91ffb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["100%|████████████████████████████████████████| 461M/461M [00:02<00:00, 165MiB/s]\n","(…)japanese-Llama-2-7b-instruct-q4_K_S.gguf: 100% 3.86G/3.86G [00:26<00:00, 146MB/s]\n","ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n","ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n","ggml_init_cublas: found 1 CUDA devices:\n","  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n","llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--mmnga--ELYZA-japanese-Llama-2-7b-instruct-gguf/snapshots/2d708f9c52bde588049a494e95b986f5bedba76f/ELYZA-japanese-Llama-2-7b-instruct-q4_K_S.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = ELYZA-japanese-Llama-2-7b-instruct\n","llama_model_loader: - kv   2:       general.source.hugginface.repository str              = elyza/ELYZA-japanese-Llama-2-7b-instruct\n","llama_model_loader: - kv   3:                   llama.tensor_data_layout str              = Meta AI original pth\n","llama_model_loader: - kv   4:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   5:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   8:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - kv  20:                          general.file_type u32              = 14\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  217 tensors\n","llama_model_loader: - type q5_K:    8 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 32\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 4096\n","llm_load_print_meta: n_embd_v_gqa     = 4096\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 11008\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q4_K - Small\n","llm_load_print_meta: model params     = 6.74 B\n","llm_load_print_meta: model size       = 3.59 GiB (4.58 BPW) \n","llm_load_print_meta: general.name     = ELYZA-japanese-Llama-2-7b-instruct\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.22 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =    70.31 MiB\n","llm_load_tensors:      CUDA0 buffer size =  3607.06 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n","llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n","llama_new_context_with_model:  CUDA_Host input buffer size   =     0.20 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =     2.68 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =     0.14 MiB\n","llama_new_context_with_model: graph splits (measure): 3\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n","Model metadata: {'general.file_type': '14', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'ELYZA-japanese-Llama-2-7b-instruct', 'general.source.hugginface.repository': 'elyza/ELYZA-japanese-Llama-2-7b-instruct', 'llama.embedding_length': '4096', 'llama.tensor_data_layout': 'Meta AI original pth', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama'}\n","Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://a277edbe43ab3c1e7d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","  新横浜は神奈川県横浜市南区新横浜町にあります。\n","llama_print_timings:        load time =     265.92 ms\n","llama_print_timings:      sample time =      24.32 ms /    38 runs   (    0.64 ms per token,  1562.69 tokens per second)\n","llama_print_timings: prompt eval time =    1072.30 ms /    90 tokens (   11.91 ms per token,    83.93 tokens per second)\n","llama_print_timings:        eval time =     851.61 ms /    37 runs   (   23.02 ms per token,    43.45 tokens per second)\n","llama_print_timings:       total time =    2163.68 ms /   127 tokens\n","Downloading: \"https://github.com/r9y9/open_jtalk/releases/download/v1.11.1/open_jtalk_dic_utf_8-1.11.tar.gz\"\n","dic.tar.gz: 100% 22.6M/22.6M [00:00<00:00, 29.6MB/s]\n","Extracting tar file /usr/local/lib/python3.10/dist-packages/pyopenjtalk/dic.tar.gz\n","Llama.generate: prefix-match hit\n","  新幹線とは、日本で開発された高速鉄道のことです。\n","\n","日本においては、東海道新幹線、山形新幹線、九州新幹線の3路線が開業しており、最高速度は320 km/h (東海道新幹線) になります。\n","\n","なお、海外においても同様の鉄道路線を新幹線と呼ぶ場合があります。\n","llama_print_timings:        load time =     265.92 ms\n","llama_print_timings:      sample time =     102.25 ms /   143 runs   (    0.72 ms per token,  1398.51 tokens per second)\n","llama_print_timings: prompt eval time =     877.74 ms /    80 tokens (   10.97 ms per token,    91.14 tokens per second)\n","llama_print_timings:        eval time =    3306.92 ms /   142 runs   (   23.29 ms per token,    42.94 tokens per second)\n","llama_print_timings:       total time =    5030.19 ms /   222 tokens\n","Keyboard interruption in main thread... closing server.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2233, in block_thread\n","    time.sleep(0.1)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n","    exec(code, run_globals)\n","  File \"/content/on-dev-chat-jp-main/ondevchatjp/free_chat.py\", line 113, in <module>\n","    demo.queue().launch(inbrowser=args.inbrowser, share=args.share)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2140, in launch\n","    self.block_thread()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2237, in block_thread\n","    self.server.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 76, in close\n","    self.thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","Killing tunnel 127.0.0.1:7860 <> https://a277edbe43ab3c1e7d.gradio.live\n","^C\n"]}]},{"cell_type":"code","source":["!python -m ondevchatjp.rag_chat --share"],"metadata":{"id":"hPwWnHVcipph","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706740434051,"user_tz":-540,"elapsed":483234,"user":{"displayName":"T F","userId":"15651270806090796677"}},"outputId":"5f61baf5-e498-4ca4-cfd3-98df518dedbd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","0it [00:00, ?it/s]\n","modules.json: 100% 341/341 [00:00<00:00, 1.83MB/s]\n","config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 511kB/s]\n","README.md: 100% 2.69k/2.69k [00:00<00:00, 15.2MB/s]\n","sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 313kB/s]\n","config.json: 100% 610/610 [00:00<00:00, 3.89MB/s]\n","pytorch_model.bin: 100% 539M/539M [00:05<00:00, 94.9MB/s]\n","tokenizer_config.json: 100% 531/531 [00:00<00:00, 2.84MB/s]\n","vocab.txt: 100% 996k/996k [00:00<00:00, 50.7MB/s]\n","tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 8.02MB/s]\n","special_tokens_map.json: 100% 112/112 [00:00<00:00, 629kB/s]\n","1_Pooling/config.json: 100% 190/190 [00:00<00:00, 927kB/s]\n","pytorch_model.bin:   0% 0.00/1.58M [00:00<?, ?B/s]\n","2_Dense/config.json: 100% 114/114 [00:00<00:00, 537kB/s]\n","pytorch_model.bin: 100% 1.58M/1.58M [00:00<00:00, 66.0MB/s]\n","ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n","ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n","ggml_init_cublas: found 1 CUDA devices:\n","  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n","llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--mmnga--ELYZA-japanese-Llama-2-7b-instruct-gguf/snapshots/2d708f9c52bde588049a494e95b986f5bedba76f/ELYZA-japanese-Llama-2-7b-instruct-q4_K_S.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = ELYZA-japanese-Llama-2-7b-instruct\n","llama_model_loader: - kv   2:       general.source.hugginface.repository str              = elyza/ELYZA-japanese-Llama-2-7b-instruct\n","llama_model_loader: - kv   3:                   llama.tensor_data_layout str              = Meta AI original pth\n","llama_model_loader: - kv   4:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   5:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   8:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - kv  20:                          general.file_type u32              = 14\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  217 tensors\n","llama_model_loader: - type q5_K:    8 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 32\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 4096\n","llm_load_print_meta: n_embd_v_gqa     = 4096\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 11008\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q4_K - Small\n","llm_load_print_meta: model params     = 6.74 B\n","llm_load_print_meta: model size       = 3.59 GiB (4.58 BPW) \n","llm_load_print_meta: general.name     = ELYZA-japanese-Llama-2-7b-instruct\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.22 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =    70.31 MiB\n","llm_load_tensors:      CUDA0 buffer size =  3607.06 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n","llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n","llama_new_context_with_model:  CUDA_Host input buffer size   =     0.20 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =     2.68 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =     0.14 MiB\n","llama_new_context_with_model: graph splits (measure): 3\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n","Model metadata: {'general.file_type': '14', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'ELYZA-japanese-Llama-2-7b-instruct', 'general.source.hugginface.repository': 'elyza/ELYZA-japanese-Llama-2-7b-instruct', 'llama.embedding_length': '4096', 'llama.tensor_data_layout': 'Meta AI original pth', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama'}\n","Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://48cd2ccca73e9ba89b.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","  1958年10月1日\n","llama_print_timings:        load time =     180.05 ms\n","llama_print_timings:      sample time =       7.70 ms /    13 runs   (    0.59 ms per token,  1689.19 tokens per second)\n","llama_print_timings: prompt eval time =    9800.87 ms /   990 tokens (    9.90 ms per token,   101.01 tokens per second)\n","llama_print_timings:        eval time =     308.03 ms /    12 runs   (   25.67 ms per token,    38.96 tokens per second)\n","llama_print_timings:       total time =   10583.02 ms /  1002 tokens\n","Llama.generate: prefix-match hit\n","  uniの特徴について、以下に前提条件の情報を元に回答します。\n","\n","uniの特徴は、子どもたちのあこがれとなったことです。\n","llama_print_timings:        load time =     180.05 ms\n","llama_print_timings:      sample time =      41.90 ms /    64 runs   (    0.65 ms per token,  1527.37 tokens per second)\n","llama_print_timings: prompt eval time =     204.80 ms /    11 tokens (   18.62 ms per token,    53.71 tokens per second)\n","llama_print_timings:        eval time =    1619.06 ms /    63 runs   (   25.70 ms per token,    38.91 tokens per second)\n","llama_print_timings:       total time =    2187.18 ms /    74 tokens\n","Keyboard interruption in main thread... closing server.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2233, in block_thread\n","    time.sleep(0.1)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n","    exec(code, run_globals)\n","  File \"/content/on-dev-chat-jp-main/ondevchatjp/rag_chat.py\", line 132, in <module>\n","    demo.queue().launch(inbrowser=args.inbrowser, share=args.share)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2140, in launch\n","    self.block_thread()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2237, in block_thread\n","    self.server.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 76, in close\n","    self.thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","Killing tunnel 127.0.0.1:7860 <> https://48cd2ccca73e9ba89b.gradio.live\n"]}]},{"cell_type":"code","source":["!python -m ondevchatjp.vector_search --share"],"metadata":{"id":"Mul4xt1Hjc7q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706740567519,"user_tz":-540,"elapsed":124195,"user":{"displayName":"T F","userId":"15651270806090796677"}},"outputId":"ea1c8ace-a5db-4243-ce21-e7855b55488e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://07c303aacf9bb39374.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","Keyboard interruption in main thread... closing server.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2233, in block_thread\n","    time.sleep(0.1)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n","    exec(code, run_globals)\n","  File \"/content/on-dev-chat-jp-main/ondevchatjp/vector_search.py\", line 143, in <module>\n","    demo.queue().launch(inbrowser=args.inbrowser, share=args.share)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2140, in launch\n","    self.block_thread()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2237, in block_thread\n","    self.server.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 76, in close\n","    self.thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","Killing tunnel 127.0.0.1:7860 <> https://07c303aacf9bb39374.gradio.live\n"]}]}]}